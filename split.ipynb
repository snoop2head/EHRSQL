{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths for database, results and scoring program\n",
    "DB_ID = 'mimic_iv'\n",
    "BASE_DATA_DIR = '/home/yjahn/joint-learning/data/mimic_iv/'\n",
    "RESULT_DIR = 'sample_result_submission/'\n",
    "SCORE_PROGRAM_DIR = 'scoring_program/'\n",
    "\n",
    "# File paths for the dataset and labels\n",
    "TABLES_PATH = os.path.join('data', DB_ID, 'tables.json')               # JSON containing database schema\n",
    "TRAIN_DATA_PATH = os.path.join(BASE_DATA_DIR, 'train', 'data.json')    # JSON file with natural language questions for training data\n",
    "TRAIN_LABEL_PATH = os.path.join(BASE_DATA_DIR, 'train', 'label.json')  # JSON file with corresponding SQL queries for training data\n",
    "VALID_DATA_PATH = os.path.join(BASE_DATA_DIR, 'valid', 'data.json')    # JSON file for validation data\n",
    "DB_PATH = os.path.join('data', DB_ID, f'{DB_ID}.sqlite')               # Database path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 5124 entries, Train labels: 5124 entries\n",
      "Valid data: 1163 entries\n"
     ]
    }
   ],
   "source": [
    "from utils import read_json as read_data\n",
    "from utils import write_json as write_data\n",
    "\n",
    "# Load train and validation sets\n",
    "train_data = read_data(TRAIN_DATA_PATH)\n",
    "train_label = read_data(TRAIN_LABEL_PATH)\n",
    "valid_data = read_data(VALID_DATA_PATH)\n",
    "\n",
    "# Quick summary of the dataset\n",
    "print(f\"Train data: {len(train_data['data'])} entries, Train labels: {len(train_label)} entries\")\n",
    "print(f\"Valid data: {len(valid_data['data'])} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified kfold\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# Create a stratified kfold split\n",
    "X = np.array(train_data['data'])\n",
    "y = []\n",
    "for key, val in train_label.items():\n",
    "    if val == \"null\":\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "New Train data: 4611 entries, New Train labels: 4611 entries, Unanswerable: 405\n",
      "New Valid data: 513 entries, New Valid labels: 513 entries, Unanswerable: 45\n",
      "Fold 2\n",
      "New Train data: 4611 entries, New Train labels: 4611 entries, Unanswerable: 405\n",
      "New Valid data: 513 entries, New Valid labels: 513 entries, Unanswerable: 45\n",
      "Fold 3\n",
      "New Train data: 4611 entries, New Train labels: 4611 entries, Unanswerable: 405\n",
      "New Valid data: 513 entries, New Valid labels: 513 entries, Unanswerable: 45\n",
      "Fold 4\n",
      "New Train data: 4611 entries, New Train labels: 4611 entries, Unanswerable: 405\n",
      "New Valid data: 513 entries, New Valid labels: 513 entries, Unanswerable: 45\n",
      "Fold 5\n",
      "New Train data: 4612 entries, New Train labels: 4612 entries, Unanswerable: 405\n",
      "New Valid data: 512 entries, New Valid labels: 512 entries, Unanswerable: 45\n",
      "Fold 6\n",
      "New Train data: 4612 entries, New Train labels: 4612 entries, Unanswerable: 405\n",
      "New Valid data: 512 entries, New Valid labels: 512 entries, Unanswerable: 45\n",
      "Fold 7\n",
      "New Train data: 4612 entries, New Train labels: 4612 entries, Unanswerable: 405\n",
      "New Valid data: 512 entries, New Valid labels: 512 entries, Unanswerable: 45\n",
      "Fold 8\n",
      "New Train data: 4612 entries, New Train labels: 4612 entries, Unanswerable: 405\n",
      "New Valid data: 512 entries, New Valid labels: 512 entries, Unanswerable: 45\n",
      "Fold 9\n",
      "New Train data: 4612 entries, New Train labels: 4612 entries, Unanswerable: 405\n",
      "New Valid data: 512 entries, New Valid labels: 512 entries, Unanswerable: 45\n",
      "Fold 10\n",
      "New Train data: 4612 entries, New Train labels: 4612 entries, Unanswerable: 405\n",
      "New Valid data: 512 entries, New Valid labels: 512 entries, Unanswerable: 45\n"
     ]
    }
   ],
   "source": [
    "# Split the data into 10 folds\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold {i+1}\")\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    new_train_keys = [sample['id'] for sample in X_train]\n",
    "    new_valid_keys = [sample['id'] for sample in X_valid]\n",
    "    \n",
    "    new_train_data = []\n",
    "    new_train_label = {}\n",
    "    new_valid_data = []\n",
    "    new_valid_label = {}\n",
    "\n",
    "    # Sort each sample into the new training or validation set as determined by the split\n",
    "    for sample in train_data['data']:\n",
    "        if sample['id'] in new_train_keys:\n",
    "            new_train_data.append(sample)\n",
    "            new_train_label[sample['id']] = train_label[sample['id']]\n",
    "        elif sample['id'] in new_valid_keys:\n",
    "            new_valid_data.append(sample)\n",
    "            new_valid_label[sample['id']] = train_label[sample['id']]\n",
    "        else:\n",
    "            # If a sample is neither in the train nor valid keys, raise an error\n",
    "            raise ValueError(f\"Error: Sample with ID {sample['id']} has an invalid split.\")\n",
    "\n",
    "    # Structure the new datasets in a JSON-compatible format\n",
    "    new_train_data = {'version': f'{DB_ID}_sample', 'data': new_train_data}\n",
    "    new_valid_data = {'version': f'{DB_ID}_sample', 'data': new_valid_data}\n",
    "\n",
    "    # Display the size of the new training and validation sets for verification\n",
    "    print(f\"New Train data: {len(new_train_data['data'])} entries, New Train labels: {len(new_train_label)} entries, Unanswerable: {sum(value == 'null' for value in new_train_label.values())}\")\n",
    "    print(f\"New Valid data: {len(new_valid_data['data'])} entries, New Valid labels: {len(new_valid_label)} entries, Unanswerable: {sum(value == 'null' for value in new_valid_label.values())}\")\n",
    "\n",
    "    # Set directory for the new splitted data\n",
    "    NEW_TRAIN_DIR = os.path.join(BASE_DATA_DIR, f'__train_fold{i}')\n",
    "    NEW_VALID_DIR = os.path.join(BASE_DATA_DIR, f'__valid_fold{i}')\n",
    "\n",
    "    # # Save the new datasets to JSON files for later use\n",
    "    write_data(os.path.join(NEW_TRAIN_DIR, \"data.json\"), new_train_data)\n",
    "    write_data(os.path.join(NEW_TRAIN_DIR, \"label.json\"), new_train_label)\n",
    "    write_data(os.path.join(NEW_VALID_DIR, \"data.json\"), new_valid_data)\n",
    "    write_data(os.path.join(NEW_VALID_DIR, \"label.json\"), new_valid_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
